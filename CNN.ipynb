{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cd3d8c9",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c935033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn.data_utils as du\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f857ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('csvTrainImages 13440x1024.csv', header = None).to_numpy()\n",
    "y_train = pd.read_csv('csvTrainLabel 13440x1.csv', header = None).to_numpy() - 1\n",
    "X_test = pd.read_csv('csvTestImages 3360x1024.csv', header = None).to_numpy()\n",
    "y_test = pd.read_csv('csvTestLabel 3360x1.csv', header = None).to_numpy() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "122eeace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13440, 1024), (13440, 1), (3360, 1024), (3360, 1))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45162888",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e132789a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13440, 32, 32, 1), (3360, 32, 32, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.reshape(-1, 32, 32, 1)\n",
    "X_test = X_test.reshape(-1, 32, 32, 1)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef82ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of outputs :  28\n",
      "Output classes :  [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(y_train)\n",
    "nClasses = len(classes)\n",
    "print('Total number of outputs : ', nClasses)\n",
    "print('Output classes : ', classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9420911",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = du.to_categorical(y_train, nClasses)\n",
    "y_test = du.to_categorical(y_test, nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ea14d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2, random_state = 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cbe12f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10752, 32, 32, 1), (2688, 32, 32, 1), (10752, 28), (2688, 28))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "85156c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_classes = nClasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c1e4377",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "numpy() is only available when eager execution is enabled.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb Cell 12\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39madd(LeakyReLU(alpha \u001b[39m=\u001b[39m alpha[elem]))                  \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb#X13sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m model\u001b[39m.\u001b[39madd(Dense(num_classes, activation \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m model\u001b[39m.\u001b[39;49mcompile(loss \u001b[39m=\u001b[39;49m keras\u001b[39m.\u001b[39;49mlosses\u001b[39m.\u001b[39;49mcategorical_crossentropy, optimizer \u001b[39m=\u001b[39;49m keras\u001b[39m.\u001b[39;49moptimizers\u001b[39m.\u001b[39;49mAdam(), metrics \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39maccuracy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m model_train \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(X_train, y_train, batch_size \u001b[39m=\u001b[39m batch_size, epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, validation_data \u001b[39m=\u001b[39m (X_val, y_val))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hkrukauskas/Documents/GitHub/ML-final-project/CNN.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mmodel_train.h5py\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training_v1.py:319\u001b[0m, in \u001b[0;36mModel.compile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_eagerly:\n\u001b[1;32m    313\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    314\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSession keyword arguments are not supported \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    315\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhen `run_eagerly=True`. You passed the following \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    316\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSession arguments: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_kwargs,)\n\u001b[1;32m    317\u001b[0m         )\n\u001b[0;32m--> 319\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_optimizer(optimizer)\n\u001b[1;32m    320\u001b[0m is_any_keras_optimizer_v1 \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(\n\u001b[1;32m    321\u001b[0m     (\n\u001b[1;32m    322\u001b[0m         \u001b[39misinstance\u001b[39m(opt, optimizer_v1\u001b[39m.\u001b[39mOptimizer)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer)\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    329\u001b[0m     is_any_keras_optimizer_v1\n\u001b[1;32m    330\u001b[0m     \u001b[39mand\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[1;32m    331\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/engine/training_v1.py:1471\u001b[0m, in \u001b[0;36mModel._set_optimizer\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m [optimizers\u001b[39m.\u001b[39mget(opt) \u001b[39mfor\u001b[39;00m opt \u001b[39min\u001b[39;00m optimizer]\n\u001b[1;32m   1470\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1471\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizers\u001b[39m.\u001b[39;49mget(optimizer)\n\u001b[1;32m   1473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dtype_policy\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmixed_float16\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1474\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, loss_scale_optimizer\u001b[39m.\u001b[39mLossScaleOptimizer\n\u001b[1;32m   1475\u001b[0m ):\n\u001b[1;32m   1476\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/__init__.py:270\u001b[0m, in \u001b[0;36mget\u001b[0;34m(identifier, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[39mreturn\u001b[39;00m identifier\n\u001b[1;32m    268\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m         \u001b[39m# If TF2 is disabled, we convert to the legacy optimizer.\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m convert_to_legacy_optimizer(identifier)\n\u001b[1;32m    272\u001b[0m \u001b[39m# Wrap legacy TF optimizer instances\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(identifier, tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mOptimizer):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/__init__.py:213\u001b[0m, in \u001b[0;36mconvert_to_legacy_optimizer\u001b[0;34m(optimizer)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`convert_to_legacy_optimizer` should only be called \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mon instances of `tf.keras.optimizers.Optimizer`, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    210\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mreceived \u001b[39m\u001b[39m{\u001b[39;00moptimizer\u001b[39m}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(optimizer)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    211\u001b[0m     )\n\u001b[1;32m    212\u001b[0m optimizer_name \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mlower()\n\u001b[0;32m--> 213\u001b[0m config \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39;49mget_config()\n\u001b[1;32m    214\u001b[0m \u001b[39m# Remove fields that only exist in experimental optimizer.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m keys_to_remove \u001b[39m=\u001b[39m [\n\u001b[1;32m    216\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mweight_decay\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    217\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muse_ema\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mis_legacy_optimizer\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m ]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/adam.py:207\u001b[0m, in \u001b[0;36mAdam.get_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_config\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    203\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_config()\n\u001b[1;32m    205\u001b[0m     config\u001b[39m.\u001b[39mupdate(\n\u001b[1;32m    206\u001b[0m         {\n\u001b[0;32m--> 207\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_serialize_hyperparameter(\n\u001b[1;32m    208\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learning_rate\n\u001b[1;32m    209\u001b[0m             ),\n\u001b[1;32m    210\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbeta_1\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1,\n\u001b[1;32m    211\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mbeta_2\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2,\n\u001b[1;32m    212\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon,\n\u001b[1;32m    213\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mamsgrad,\n\u001b[1;32m    214\u001b[0m         }\n\u001b[1;32m    215\u001b[0m     )\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m config\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/optimizers/optimizer_experimental/optimizer.py:718\u001b[0m, in \u001b[0;36m_BaseOptimizer._serialize_hyperparameter\u001b[0;34m(self, hyperparameter)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[39mreturn\u001b[39;00m learning_rate_schedule\u001b[39m.\u001b[39mserialize(hyperparameter)\n\u001b[1;32m    717\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(hyperparameter, tf\u001b[39m.\u001b[39mVariable):\n\u001b[0;32m--> 718\u001b[0m     \u001b[39mreturn\u001b[39;00m hyperparameter\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m callable(hyperparameter):\n\u001b[1;32m    720\u001b[0m     \u001b[39mreturn\u001b[39;00m hyperparameter()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:640\u001b[0m, in \u001b[0;36mBaseResourceVariable.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    639\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_value()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m--> 640\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    641\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mnumpy() is only available when eager execution is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: numpy() is only available when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "alpha = [0.0001, 0.001, 0.1, 1, 10, 100]\n",
    "lst = []\n",
    "i=0\n",
    "for elem in range(len(alpha)):\n",
    "    print(alpha[elem])\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = (3, 3), activation = 'tanh', input_shape = (32, 32, 1), padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = alpha[elem]))\n",
    "    model.add(MaxPooling2D((2, 2), padding = 'same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation = 'tanh', padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = alpha[elem]))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2), padding = 'same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation = 'tanh', padding = 'same'))\n",
    "    model.add(LeakyReLU(alpha = alpha[elem]))                  \n",
    "    model.add(MaxPooling2D(pool_size = (2, 2), padding = 'same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'tanh'))\n",
    "    model.add(LeakyReLU(alpha = alpha[elem]))                  \n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "\n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, optimizer = keras.optimizers.Adam(), metrics = ['accuracy'])\n",
    "\n",
    "    model_train = model.fit(X_train, y_train, batch_size = batch_size, epochs = 5, verbose = 1, validation_data = (X_val, y_val))\n",
    "\n",
    "    model.save(\"model_train.h5py\")\n",
    "\n",
    "    accuracy = model_train.history['acc']\n",
    "    val_accuracy = model_train.history['val_acc']\n",
    "    loss = model_train.history['loss']\n",
    "    val_loss = model_train.history['val_loss']\n",
    "    epochs = list(range(len(accuracy)))\n",
    "    lst.append((val_accuracy[-1], alpha[i]))\n",
    "    i += 1\n",
    "    plt.plot(epochs, accuracy, 'bo', label = 'Training accuracy')\n",
    "    plt.plot(epochs, val_accuracy, 'b', label = 'Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf22fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "act = ['tanh', 'sigmoid', 'relu', 'linear']\n",
    "lst = []\n",
    "i=0\n",
    "for elem in range(len(act)):\n",
    "    print(act[elem])\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),activation=act[elem],input_shape=(32,32,1),padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "    model.add(Conv2D(64, (3, 3), activation=act[elem],padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    model.add(Conv2D(128, (3, 3), activation=act[elem],padding='same'))                 \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation=act[elem]))              \n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "    model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "    model.save(\"model_train.h5py\")\n",
    "\n",
    "    accuracy = model_train.history['acc']\n",
    "    val_accuracy = model_train.history['val_acc']\n",
    "    loss = model_train.history['loss']\n",
    "    val_loss = model_train.history['val_loss']\n",
    "    epochs = list(range(len(accuracy)))\n",
    "    lst.append((val_accuracy[-1], act[i]))\n",
    "    i += 1\n",
    "    plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "    plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "print(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e371c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = [0.0001, 0.001, 0.1, 1, 10, 100]\n",
    "act = ['tanh', 'sigmoid', 'relu', 'linear']\n",
    "lst = []\n",
    "i = 0\n",
    "for j in act:\n",
    "    for elem in range(len(l2)):\n",
    "        print((l2[elem], j))\n",
    "        model = Sequential()\n",
    "        model.add(Convacc2D(32, kernel_size=(3, 3),activation=j,input_shape=(32,32,1),padding='same'))\n",
    "        model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "        model.add(Conv2D(64, (3, 3), activation=j,padding='same', kernel_regularizer =tf.keras.regularizers.l2( l=l2[elem])))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "        model.add(Conv2D(128, (3, 3), activation=j,padding='same', kernel_regularizer =tf.keras.regularizers.l2( l=l2[elem])))                 \n",
    "        model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation=j))              \n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "        model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "        model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "        model.save(\"model_train.h5py\")\n",
    "\n",
    "        accuracy = model_train.history['acc']\n",
    "        val_accuracy = model_train.history['val_acc']\n",
    "        loss = model_train.history['loss']\n",
    "        val_loss = model_train.history['val_loss']\n",
    "        epochs = list(range(len(accuracy)))\n",
    "        plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "        plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "        plt.title('Training and validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "        plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ff551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,1),padding='same'))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))              \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "model.save(\"model_train.h5py\")\n",
    "\n",
    "accuracy = model_train.history['acc']\n",
    "val_accuracy = model_train.history['val_acc']\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = list(range(len(accuracy)))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0234f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,1),padding='same'))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))              \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "model.save(\"model_train.h5py\")\n",
    "\n",
    "accuracy = model_train.history['acc']\n",
    "val_accuracy = model_train.history['val_acc']\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = list(range(len(accuracy)))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bda53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,1),padding='same'))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='linear',padding='same'))                 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))                 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))                 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))                 \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))              \n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "model.save(\"model_train.h5py\")\n",
    "\n",
    "accuracy = model_train.history['acc']\n",
    "val_accuracy = model_train.history['val_acc']\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = list(range(len(accuracy)))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61621b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(32,32,1),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_train = model.fit(X_train, y_train, batch_size=batch_size,epochs=5,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "model.save(\"model_train.h5py\")\n",
    "\n",
    "accuracy = model_train.history['acc']\n",
    "val_accuracy = model_train.history['val_acc']\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = list(range(len(accuracy)))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# layer 1\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='tanh',input_shape=(32,32,1),padding='same'))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "# layer 2\n",
    "model.add(Conv2D(64, (3, 3), activation='tanh',padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# layer 3\n",
    "model.add(Conv2D(128, (3, 3), activation='tanh',padding='same'))  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "# layer 4\n",
    "model.add(Dense(128, activation='tanh'))\n",
    "# layer 5\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n",
    "\n",
    "model_train = model.fit(X_train, y_train, batch_size=64,epochs=13,verbose=1,validation_data=(X_val, y_val))\n",
    "\n",
    "model.save(\"model_train.h5py\")\n",
    "\n",
    "accuracy = model_train.history['acc']\n",
    "val_accuracy = model_train.history['val_acc']\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = list(range(len(accuracy)))\n",
    "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237556db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"Class {}\".format(i) for i in range(num_classes)]\n",
    "print(classification_report(y_true, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36903921",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions = np.argmax(predictions, axis = 1)\n",
    "y_true = np.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, predictions)\n",
    "plt.figure(figsize=(10,15))\n",
    "sn.heatmap(cm, annot=True, fmt=\"d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e980ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = sum(cm[i][i] for i in range(28)) / y_true.shape[0]\n",
    "print(\"accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8599903",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
